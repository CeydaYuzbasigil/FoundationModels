{"cells":[{"cell_type":"markdown","metadata":{"id":"-UzfGAYMDLBE"},"source":["## Tokenization with byte pair encoding\n","\n","This notebook, lecture, and homework is based on Anrey Karpathy's excellent online lecture on tokenizers:\n","https://www.youtube.com/watch?v=zduSFxRajkE"]},{"cell_type":"markdown","metadata":{"id":"618iWEcTDLBG"},"source":["## Byte Pair Encoding\n","\n","The original Byte Pair Encoding (BPE) was introduced for data compression and iteratively replaces the most commen sequence of characters with a new variable.\n","\n","Here is the example from https://en.wikipedia.org/wiki/Byte_pair_encoding:\n","\n","\n","Suppose the data string is: \"aaabdaaabac\".\n","\n","The counts of the byte pairs are:\n","(aa, 5),  (ab, 2), (bd, 1), (da, 1), (ac, 1)\n","\n","The most frequent byte pair is aa, so it is replaced with an ununsed byte, say Z. This yields:\n","\n","ZabdZabac  \n","Z = aa\n","\n","For this data string, the most frequent pair is ab, which is replaced with Y, yielding:\n","\n","ZYdZYac  \n","Z = aa  \n","Y = ab\n","\n","For this data string, the most frequent pair is ZY, replaced by X yields:\n","\n","XdXac  \n","Z = aa  \n","Y = ab  \n","X = ZY\n","\n","Now there is no sequence that appears more than once.\n","\n","BPE for natural language processing (NLP) works analogously, only that we start with a dictionary which initially consists of the byte valies of the 256 ASCII characters. Then we add values 256,257, ... and so on by merging the most frequent pairs.\n","\n","\n","## History of BPE\n","\n","BPE for language modeling was introduced by Sennrich et al., ``Neural Machine Translation of Rare Words with Subword Units''.\n","\n","The paper writes as it's main contributions:\n","- \"We show that open-vocabulary neural machine translation is possible by encoding (rare) words via subword units.\"\n","- \"We adapt byte pair encoding (BPE) (Gage, 1994), a compression algorithm, to the task of word segmentation. BPE allows for the representation of an open vocabulary through a fixed-size vocabulary of variable-length character sequences, making it a very suitable word segmentation strategy for neural network models''\n","\n","The GPT2 paper used a BPE encoding algorithm and justifies the choice as follows:\n","\n","\"A general language model (LM) should be able to compute the probability of (and also generate) any string. Current large scale LMs include pre-processing steps such as lowercasing, tokenization, and out-of-vocabulary tokens which restrict the space of model-able strings. While processing Unicode strings as a sequence of UTF-8 bytes elegantly fulfills this requirement as exemplified in work such as Gillick et al. (2015), current byte-level LMs are not competitive with word-level LMs on large scale datasets such as the One Billion Word Benchmark (Al-Rfou et al., 2018). We observed a similar performance gap in our own attempts to train standard byte-level LMs on WebText.\n","\n","Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences. Despite its name, reference BPE implementations often operate on Unicode code points and not byte sequences. These implementations would require including the full space of Unicode symbols in order to model all Unicode strings. This would result in a base vocabulary of over 130,000 before any multi-symbol tokens are added. This is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with BPE. In contrast, a byte-level version of BPE only requires a base vocabulary of size 256.\"\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jXD2ZXLpDLBI"},"source":["# Step-by-step to a tokenizer\n","\n","### Converting characters to bytes\n","\n","Charaters are typically encoded using UTF-8, a character encoding that encodes all possible characters in Unicode as a sequence of one to four bytes. UTF-8 is designed to be backward compatible with ASCII for the first 128 characters, making it efficient for texts where these characters are predominant. UTF-8 is the dominant encoding for the web and many computing systems because it can represent any character in the Unicode standard, yet is space-efficient for texts primarily using Latin characters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVPX3Xv1DLBK","executionInfo":{"status":"ok","timestamp":1748187054266,"user_tz":-120,"elapsed":21,"user":{"displayName":"Ceyda YÃ¼zbaÅŸÄ±gil","userId":"12368585341661515151"}},"outputId":"c3c63d12-2bd9-4e14-dbe6-03af82979559"},"outputs":[{"output_type":"stream","name":"stdout","text":["b'Hello World!' , number of bytes:  12\n","b'\\xc3\\xb6' , number of bytes:  2\n","b'\\xf0\\x9f\\x98\\x80' , number of bytes:  4\n","String in previous example as tokens: [97, 97, 97, 98, 100, 97, 97, 97, 98, 97, 99]\n"]}],"source":["# Convert text to bytes\n","\n","text = \"Hello World!\"\n","tokens = text.encode('utf-8') # convert text to bytes\n","print( tokens, \", number of bytes: \", len(tokens) ) # one token per character\n","\n","text = \"Ã¶\"\n","tokens = text.encode('utf-8') # convert text to bytes\n","print( tokens, \", number of bytes: \", len(tokens) ) # two token for this special character\n","\n","text = \"ðŸ˜€\"\n","tokens = text.encode('utf-8') # convert text to bytes\n","print( tokens, \", number of bytes: \", len(tokens) ) # four token for this special character\n","\n","\n","# Let's use the same string as in the previous example for illustrating BPE\n","\n","text = \"aaabdaaabac\"\n","tokens = text.encode('utf-8') # convert text to bytes\n","tokens = list(map(int, tokens)) # convert bytes to list of integers\n","print(f\"String in previous example as tokens: {tokens}\")\n"]},{"cell_type":"markdown","metadata":{"id":"xXa6QTFSDLBM"},"source":["### Computing frequencies of consequitive bytes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cf4FbzjRDLBM","executionInfo":{"status":"ok","timestamp":1748187237897,"user_tz":-120,"elapsed":9,"user":{"displayName":"Ceyda YÃ¼zbaÅŸÄ±gil","userId":"12368585341661515151"}},"outputId":"408fdad1-bca8-4ccc-d093-389e5974ed94"},"outputs":[{"output_type":"stream","name":"stdout","text":["{(97, 97): 4, (97, 98): 2, (98, 100): 1, (100, 97): 1, (98, 97): 1, (97, 99): 1}\n"]}],"source":["def get_frequencies(seq):\n","    counts = {}\n","    for pair in zip(seq,seq[1:]): # iterate over consequitive elements\n","        counts[pair] = counts.get(pair,0) + 1\n","    return counts\n","\n","frequencies = get_frequencies(tokens)\n","\n","sorted(frequencies.items(), key=lambda x: x[1], reverse=True)\n","\n","print(frequencies)"]},{"cell_type":"markdown","metadata":{"id":"FVmv36ytDLBO"},"source":["### Merging function"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SmgK3xsPDLBP","executionInfo":{"status":"ok","timestamp":1748187367080,"user_tz":-120,"elapsed":8,"user":{"displayName":"Ceyda YÃ¼zbaÅŸÄ±gil","userId":"12368585341661515151"}},"outputId":"27cf606d-97cd-46fe-ce8a-e1bec530e999"},"outputs":[{"output_type":"stream","name":"stdout","text":["(97, 97)\n","[97, 97, 97, 98, 100, 97, 97, 97, 98, 97, 99]\n","[256, 97, 98, 100, 256, 97, 98, 97, 99]\n"]}],"source":["top_pair = max(frequencies.items(), key=lambda x: x[1])[0]\n","print(top_pair)\n","\n","def merge(seq,pair,index):\n","    new_seq = []\n","    i = 0\n","    while i < len(seq):\n","        if seq[i:i+2] == list(pair) and i < len(seq) - 1:\n","            new_seq.append(index)\n","            i += 2\n","        else:\n","            new_seq.append(seq[i])\n","            i += 1\n","    return new_seq\n","\n","new_seq = merge(tokens,top_pair,256)\n","print(tokens)\n","print(new_seq)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ykpD-xD-DLBQ"},"source":["### Tokenizer training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yOluNQWQDLBR","executionInfo":{"status":"ok","timestamp":1748187965583,"user_tz":-120,"elapsed":8,"user":{"displayName":"Ceyda YÃ¼zbaÅŸÄ±gil","userId":"12368585341661515151"}},"outputId":"7cba3489-83cd-47dc-ff3e-a61df17b6056"},"outputs":[{"output_type":"stream","name":"stdout","text":["merge (97, 97) -> 256\n","merge (256, 97) -> 257\n","merge (257, 98) -> 258\n","merge (258, 100) -> 259\n","merge (259, 258) -> 260\n"]}],"source":["vocab_size = 256+5 # desired final vocabulary size\n","num_merges = vocab_size - 256\n","seq = list(tokens) # make a copy of the original sequence\n","merges = {} # dictionary to store merges, int x int -> int\n","for i in range(num_merges):\n","    frequencies = get_frequencies(seq)\n","    pair = max(frequencies.items(), key=lambda x: x[1])[0]\n","    seq = merge(seq,pair,256+i)\n","    merges[pair] = 256+i\n","    print(f\"merge {pair} -> {256+i}\")\n","\n","\n","    \"\"\"[97, 97, 97, 98, 100, 97, 97, 97, 98, 97, 99]\n","\n","    ** 256:\n","    (97,97) , 4\n","    [256, 97, 98, 100, 256, 97, 98, 97, 99]\n","\n","    ** 257:\n","    (256,97), 2\n","    [257, 98, 100, 257, 98, 97, 99]\n","\n","    **258:\n","    (257,98), 2\n","    [258, 100, 258, 97, 99]\n","\n","    ** 259:\n","    (258,100), 1\n","    [259, 258, 97, 99]\n","\n","    ** 260:\n","    (259,258), 1\n","    [260, 97, 99]\"\"\"\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r1XFXC8lDLBS"},"source":["### Decoding\n","\n","Given a sequence of integers in the range [0,vocab_size], decoding yields the corresponding text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyfEe8ENDLBV","executionInfo":{"status":"ok","timestamp":1748187976379,"user_tz":-120,"elapsed":20,"user":{"displayName":"Ceyda YÃ¼zbaÅŸÄ±gil","userId":"12368585341661515151"}},"outputId":"484b85b2-06fa-48fd-a9f4-bc0fd342956c"},"outputs":[{"output_type":"stream","name":"stdout","text":["{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'aa', 257: b'aaa', 258: b'aaab', 259: b'aaabd', 260: b'aaabdaaab'}\n"]}],"source":["# construct the vocabulary\n","\n","# the first 172 elements of the are the ASCII characters\n","vocab = {idx: bytes([idx]) for idx in range(256)}\n","\n","# the rest of the vocabulary is constructed from the merges\n","for (i,j), idx in merges.items(): # this must be done in the same order as we filled in the merges dictionary\n","    # concatenates the bytes; for example for our first merge: b'a' + b'a' -> b'aa'\n","    vocab[idx] = vocab[i] + vocab[j]\n","\n","print(vocab)\n","\n","def decode(seq,vocab):\n","    # convert the sequence of integers to bytes, and then concatenate them\n","    tokens = b\"\".join([vocab[s] for s in seq]) # concatenate the bytes\n","    # replace option is used to replace unknown characters with a special character,\n","    # incase the LLM\n","    text = tokens.decode('utf-8',errors='replace')\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYhQV3nGDLBW","executionInfo":{"status":"ok","timestamp":1748187990329,"user_tz":-120,"elapsed":15,"user":{"displayName":"Ceyda YÃ¼zbaÅŸÄ±gil","userId":"12368585341661515151"}},"outputId":"ce79eaea-082f-40d7-fd85-b4a8cfd9940d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[260, 97, 99]\n","aaabdaaabac\n"]}],"source":["def encode(text):\n","    # convert the string to list of integers\n","    tokens = list(text.encode('utf-8'))\n","\n","    while True:\n","        freqs = get_frequencies(tokens)\n","        # find the pair that has the lowest index in the merges dictionary\n","        # since this prioeritize the pairs that were merged first\n","        # the custom key function looks up each pair's index in the merges dictionary,\n","        # and defaults to infinity if the pair is not in the merges dictionary\n","        pair = min(freqs, key=lambda x: merges.get(x,float('inf')))\n","        if pair not in merges: # nothing to merge\n","            break\n","        # merge the pair\n","        index = merges[pair]\n","        tokens = merge(tokens,pair,index)\n","    return tokens\n","\n","print(encode(\"aaabdaaabac\"))\n","\n","print(decode(encode(\"aaabdaaabac\"),vocab))\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uMGr6nMcDLBW"},"source":["### Pre-tokenization\n","\n","Pre-tokenization is the process of using a set of rules to restrict the creation of certain tokens. Pre-tokenization is often done via splitting with regular expressions before appling the BPE algorithm.\n","\n","The motivation for this is as follows, from Radford et al. \"Language Models are Unsupervised Multitask Learners\":\n","\n","\"However, directly applying BPE to the byte sequence results in suboptimal merges due to BPE using a greedy frequency based heuristic for building the token vocabulary. We observed BPE including many versions of common words like dog since they occur in many variations such as dog. dog! dog? . This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding only minimal fragmentation of words across multiple vocab tokens. This input representation allows us to combine the empirical benefits of word-level LMs with the generality of byte-level approaches. Since our approach can assign a probability to any Unicode string, this allows us to evaluate our LMs on any dataset regardless of pre-processing, tokenization, or vocab size.\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7hz-cNRSDLBX","executionInfo":{"status":"ok","timestamp":1748289253798,"user_tz":-120,"elapsed":123,"user":{"displayName":"Ceyda YÃ¼zbaÅŸÄ±gil","userId":"12368585341661515151"}},"outputId":"eeb1a3f3-3ea2-49c0-8717-517753ebe99e"},"outputs":[{"output_type":"stream","name":"stdout","text":["['He', \"'s\", ' in', ' the', ' gym', ',', ' and', ' he', \"'ll\", ' work', ' out', ' for', ' an', ' hour', '.']\n","['The', ' cost', ' is', ' 200', ',', '000', '.', '00', ' dollars', '.', ' Thats', ' 2', 'x', ' the', ' cost', ' of', ' the', ' previous', ' model', '.']\n","['He', \"'s\", ' in', ' the', ' gym', ',', ' and', ' he', \"'ll\", ' work', ' out', ' for', ' an', ' hour', '.']\n","['The', ' cost', ' is', ' ', '200', ',', '000', '.', '00', ' dollars', '.', ' Thats', ' ', '2', 'x', ' the', ' cost', ' of', ' the', ' previous', ' model', '.']\n"]}],"source":["import regex as re\n","\n","# the main GPT text split patterns, see\n","# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n","GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","\n","# the GP2 split pattern splits the text into words, numbers, punctuation, and whitespace segments.\n","# a few examples:\n","\n","print(re.findall(GPT2_SPLIT_PATTERN, \"He's in the gym, and he'll work out for an hour.\"))\n","print(re.findall(GPT2_SPLIT_PATTERN, \"The cost is 200,000.00 dollars. Thats 2x the cost of the previous model.\"))\n","\n","print(re.findall(GPT4_SPLIT_PATTERN, \"He's in the gym, and he'll work out for an hour.\"))\n","print(re.findall(GPT4_SPLIT_PATTERN, \"The cost is 200,000.00 dollars. Thats 2x the cost of the previous model.\"))\n","\n","\n","# only merges within elements in those lists are considered in the BPE algorithm"]},{"cell_type":"markdown","metadata":{"id":"SZW8byAoDLBY"},"source":["### Treatment of special tokens\n","'<|endoftext|>': markes the end of one text, gets assigned an extra tokens, is applied outside of the BPE algorithm and is not getting merged"]},{"cell_type":"markdown","metadata":{"id":"nkzBU958DLBY"},"source":["# Concluding notes\n","\n","Try this webinterface to see different tokenizers in action:\n","https://huggingface.co/spaces/Xenova/the-tokenizer-playground\n","\n","Here you can inspect the vocabulary of GPT4:\n","https://github.com/kaisugi/gpt4_vocab_list/blob/main/cl100k_base_vocab_list.txt\n","\n","Here are the vocab sizes for a few popoular language models:\n","\n","\n","| Model         |  Vocab Size |\n","|---------------|--------------|\n","| GPT-1         | 40,478     |\n","| GPT-2         | 50,304     |\n","| GPT-3 (large) | 50,257     |\n","| GPT-4         |  100,256    |\n","| LLama 2       | 32,000      |\n","| LLama 3       | 128,256     |\n","\n","\n","- BPE merges the most common pairs of tokens; another option for merging used by WordPiece is to use the pointwise mutal information instead the count to decide which tokens to merge\n","\n","- Tokenizer and language models should be trained on same or similar data. A tokenizer trained on natural language will not perform well when used for a llm for code, it will lead to sequences beeing encoded inefficiently (average of character/tokens is small) and sub-optimal llm performance\n","\n","- Good tokenizers are often efficient in that they somewhat compress text (i.e., the average of character/tokens is small).  Howerver, fewer tokens do not automatically lead to better downstream performance, we see this for example from pre-tokenization beeing efficient. See for example the paper: http://arxiv.org/abs/2402.18376\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C7cMi0qLDLBY"},"source":["# Homework\n","\n","Implement the train, decode, and encode functions of the tokenizer below.\n","The tokenizer should use the GPT2_SPLIT_PATTERN and treat the special token <|endoftext|> appropriately."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"PT9Jqg4sDLBY","executionInfo":{"status":"ok","timestamp":1748289263830,"user_tz":-120,"elapsed":36,"user":{"displayName":"Ceyda YÃ¼zbaÅŸÄ±gil","userId":"12368585341661515151"}}},"outputs":[],"source":["import regex as re\n","# -----------------------------------------------------------------------------\n","# the base Tokenizer class\n","\n","class Tokenizer:\n","    \"\"\"Base class for Tokenizers\"\"\"\n","\n","    def __init__(self):\n","        # default: vocab size of 256 (all bytes), no merges, no patterns\n","        self.merges = {} # (int, int) -> int\n","        self.pattern = \"\" # str\n","        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = self._build_vocab() # int -> bytes\n","\n","\n","    def _build_vocab(self):\n","        # vocab is simply and deterministically derived from merges\n","        vocab = {idx: bytes([idx]) for idx in range(256)}\n","        for (p0, p1), idx in self.merges.items():\n","            vocab[idx] = vocab[p0] + vocab[p1]\n","        for special, idx in self.special_tokens.items():\n","            vocab[idx] = special.encode(\"utf-8\")\n","        return vocab\n","\n","    def train(self, text, vocab_size, verbose=False):\n","        # Tokenizer can train a vocabulary of size vocab_size from text\n","\n","        # GPT2_SPLIT_PATTERN\n","\n","          # '(?:[sdmt]|ll|ve|re) means 's 'd 'm 't 'll 've 're\n","          # ?\\p{L} means all unicode letter, latin + arabic letter groups\n","          # ?\\p{N} means all unicode numbers\n","\n","          # [^   ] means exclude\n","          # all unicode letter and unicode numbers groups\n","          # are excluded.\n","          # i.e. This exception means '!', ',', '@#$%' punctuations & symbols\n","\n","          # |\\s+(?!\\S)|\\s+ matches spaces, tabs, etc.,\n","\n","          if self.pattern == \"\":\n","            global GPT2_SPLIT_PATTERN\n","            GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n","            self.pattern = GPT2_SPLIT_PATTERN\n","\n","          tokens = re.findall(self.pattern, text )\n","          tokens_byte = [token.encode('utf-8') for token in tokens]\n","\n","          merges = {}\n","          num_merges = vocab_size - 256\n","          next_id = 256\n","\n","          for _ in range(num_merges):\n","            # Count frequencies of all adjacent byte pairs\n","            pair_counts = {}\n","            for token in tokens_byte:\n","                for i in range(len(token) - 1):\n","                    pair = (token[i], token[i+1])\n","                    pair_counts[pair] = pair_counts.get(pair, 0) + 1\n","\n","            if not pair_counts:\n","                break  # No more pairs to merge\n","\n","            # Find the most frequent pair\n","            most_freq_pair = max(pair_counts, key = pair_counts.get)\n","            merges[most_freq_pair] = next_id\n","\n","            # Replace that pair in all tokens\n","            new_tokens_byte = []\n","            for token in tokens_byte:\n","                i = 0\n","                new_token = []\n","                while i < len(token):\n","                    if i < len(token) - 1 and (token[i], token[i+1]) == most_freq_pair:\n","                        new_token.append(next_id)\n","                        i += 2\n","                    else:\n","                        new_token.append(token[i])\n","                        i += 1\n","                new_tokens_byte.append(new_token)\n","\n","            tokens_byte = new_tokens_byte\n","            next_id += 1\n","\n","\n","          self.merges = merges\n","          self.special_tokens = {'<|endoftext|>': 100257}\n","          self.vocab = self._build_vocab()\n","\n","\n","        #raise NotImplementedError\n","\n","    def _apply_merges(self, byte_seq):\n","        token = list(byte_seq)\n","        merge_pairs = self.merges\n","\n","        while True:\n","            pairs = list(zip(token, token[1:]))\n","            match = None\n","            for i in range(len(pairs)):\n","                if pairs[i] in merge_pairs:\n","                    match = (i, pairs[i])\n","                    break\n","            if not match:\n","                break\n","            i, pair = match\n","            token = token[:i] + [merge_pairs[pair]] + token[i+2:]\n","        return token\n","\n","    def encode(self, text):\n","        # Tokenizer can encode a string into a list of integers\n","\n","        tokens = re.findall(self.pattern, text)\n","\n","        encoded_byte= []\n","\n","        for token in tokens:\n","            token_byte = token.encode('utf-8')\n","            merged_byte = self._apply_merges(token_byte)\n","\n","            encoded_byte.extend(merged_byte)\n","\n","        encoded_byte.append(self.special_tokens['<|endoftext|>'])\n","        return encoded_byte\n","\n","\n","    def decode(self, ids):\n","        # Tokenizer can decode a list of integers into a string\n","        decoded_byte = []\n","        decoded_text = \"\"\n","\n","        for id in ids:\n","          if id in self.vocab:\n","            decoded_byte.append(self.vocab[id])\n","\n","          else: #For an unknown id\n","            decoded_byte.append(b'\\xef\\xbf\\xbd')\n","\n","        decoded_byte = b''.join(decoded_byte)\n","        decoded_text = decoded_byte.decode('utf-8', errors='replace')\n","\n","        return decoded_text\n","\n","\n","\n","    def save(self, file_prefix):\n","        \"\"\"\n","        Saves two files: file_prefix.vocab and file_prefix.model\n","        This is inspired (but not equivalent to!) sentencepiece's model saving:\n","        - model file is the critical one, intended for load()\n","        - vocab file is just a pretty printed version for human inspection only\n","        \"\"\"\n","        # write the model: to be used in load() later\n","        model_file = file_prefix + \".model\"\n","        with open(model_file, 'w') as f:\n","            # write the version, pattern and merges, that's all that's needed\n","            f.write(\"minbpe v1\\n\")\n","            f.write(f\"{self.pattern}\\n\")\n","            # write the special tokens, first the number of them, then each one\n","            f.write(f\"{len(self.special_tokens)}\\n\")\n","            for special, idx in self.special_tokens.items():\n","                f.write(f\"{special} {idx}\\n\")\n","            # the merges dict\n","            for idx1, idx2 in self.merges:\n","                f.write(f\"{idx1} {idx2}\\n\")\n","        # write the vocab: for the human to look at\n","        vocab_file = file_prefix + \".vocab\"\n","        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n","        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n","            for idx, token in self.vocab.items():\n","                # note: many tokens may be partial utf-8 sequences\n","                # and cannot be decoded into valid strings. Here we're using\n","                # errors='replace' to replace them with the replacement char ï¿½.\n","                # this also means that we couldn't possibly use .vocab in load()\n","                # because decoding in this way is a lossy operation!\n","                s = render_token(token)\n","                # find the children of this token, if any\n","                if idx in inverted_merges:\n","                    # if this token has children, render it nicely as a merge\n","                    idx0, idx1 = inverted_merges[idx]\n","                    s0 = render_token(self.vocab[idx0])\n","                    s1 = render_token(self.vocab[idx1])\n","                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n","                else:\n","                    # otherwise this is leaf token, just print it\n","                    # (this should just be the first 256 tokens, the bytes)\n","                    f.write(f\"[{s}] {idx}\\n\")\n","\n","    def load(self, model_file):\n","        \"\"\"Inverse of save() but only for the model file\"\"\"\n","        assert model_file.endswith(\".model\")\n","        # read the model file\n","        merges = {}\n","        special_tokens = {}\n","        idx = 256\n","        with open(model_file, 'r', encoding=\"utf-8\") as f:\n","            # read the version\n","            version = f.readline().strip()\n","            assert version == \"minbpe v1\"\n","            # read the pattern\n","            self.pattern = f.readline().strip()\n","            # read the special tokens\n","            num_special = int(f.readline().strip())\n","            for _ in range(num_special):\n","                special, special_idx = f.readline().strip().split()\n","                special_tokens[special] = int(special_idx)\n","            # read the merges\n","            for line in f:\n","                idx1, idx2 = map(int, line.split())\n","                merges[(idx1, idx2)] = idx\n","                idx += 1\n","        self.merges = merges\n","        self.special_tokens = special_tokens\n","        self.vocab = self._build_vocab()"]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","text = \"Don't stop believing!\"\n","tokenizer.train(text, vocab_size=300)\n","\n","vocab_trained = tokenizer.vocab\n","print(vocab_trained)\n","\n","encoded = tokenizer.encode(text)\n","print(\"Encoded:\", encoded)\n","\n","decoded = tokenizer.decode(encoded)\n","print(\"Decoded:\", decoded)"],"metadata":{"id":"iaHrSr5uKmRU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748289302042,"user_tz":-120,"elapsed":24,"user":{"displayName":"Ceyda YÃ¼zbaÅŸÄ±gil","userId":"12368585341661515151"}},"outputId":"7d712ccd-4393-49f0-f8eb-be2d68d411ef"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'Do', 257: b'Don', 258: b\"'t\", 259: b' s', 260: b' st', 261: b' sto', 262: b' stop', 263: b' b', 264: b' be', 265: b' bel', 266: b' beli', 267: b' belie', 268: b' believ', 269: b' believi', 270: b' believin', 271: b' believing', 100257: b'<|endoftext|>'}\n","Encoded: [257, 258, 262, 271, 33, 100257]\n","Decoded: Don't stop believing!<|endoftext|>\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}